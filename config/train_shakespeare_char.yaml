# defaults:
#   - model: 'shakespeare_char'
#   - data: 'shakespeare_char'
#   - train: 'shakespeare_char'
#   - optimizer: 'shakespeare_char'
#   - scheduler: 'shakespeare_char'

# I/O
out_dir: 'out-shakespeare-char'
eval_interval: 250
log_interval: 10
eval_iters: 200
eval_only: False
always_save_checkpoint: False
init_from: 'scratch'

# wandb logging
wandb_log: True
wandb_project: 'shakespeare_char'
wandb_run_name: 'mini-gpt'

# data
dataset: 'shakespeare_char'
gradient_accumulation_steps: 1
batch_size: 64
block_size: 256

# model
n_layer: 6
n_head: 6
n_embd: 384
dropout: 0.2
bias: False

# Optimizer
optimizer:
# adamw optimizer
  learning_rate: 6e-4 # max learning rate
  max_iters: 2000 # total number of training iterations
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0 # clip gradients at this value, or disable if == 0.0
# learning rate decay settings
  decay_lr: True # whether to decay the learning rate
  warmup_iters: 100 # how many steps to warm up for
  lr_decay_iters: 5000 # should be ~= max_iters per Chinchilla
  min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

# DDP setttings
backend: 'nccl' # 'nccl', 'gloo', etc.

# system
device: 'mps'
dtype: 'bfloat16' # if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
compile: False # use PyTorch 2.0 to compile the model to be faster

# Hydra config
hydra:
  run:
    dir: ./outputs/exp_${hydra.job.override_dirname}
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know...
        exclude_keys: [restart, data.root]